{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from itertools import chain \n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from scipy.sparse import lil_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from string import punctuation\n",
    "\n",
    "from __future__ import division\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Watermark - tool to help with reproducibility:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%install_ext https://raw.githubusercontent.com/rasbt/watermark/master/watermark/watermark.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last updated: Wed Jul 13 2016 23:32:26 CDT\n",
      "\n",
      "CPython 2.7.11\n",
      "IPython 4.0.3\n",
      "\n",
      "nltk 3.0.3\n",
      "numpy 1.10.1\n",
      "scipy 0.17.0\n",
      "\n",
      "compiler   : GCC 4.2.1 (Apple Inc. build 5577)\n",
      "system     : Darwin\n",
      "release    : 15.5.0\n",
      "machine    : x86_64\n",
      "processor  : i386\n",
      "CPU cores  : 4\n",
      "interpreter: 64bit\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark -n -t -z -u -m -v -p nltk,numpy,scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_data(file_name):\n",
    "    sents = []\n",
    "    with open(file_name, 'rb') as f:\n",
    "        for line in f:\n",
    "            if line.startswith(\"<c> \"):\n",
    "                line = line.decode('cp1252') #convert from Windows Latin-1 encoding to avoid unicode issues\n",
    "                tagged_words = line.split(\" \")[1:] #skip the <c>\n",
    "                sents.append([tagged_word.split('|') for tagged_word in tagged_words])\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up the data\n",
    "#### Remove stop words, punctuation, empty strings, then lowercase and strip punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def clean_sents(sents, tag_index):\n",
    "    stopwords_set = set(stopwords.words('english'))\n",
    "    punctuation_set = set(punctuation)\n",
    "    cleaned_sents = []\n",
    "    for sent in sents:\n",
    "        cleaned_sent = []\n",
    "        for word in sent:\n",
    "            if word[1].lower() not in stopwords_set and word[1] not in punctuation_set and len(word[1].strip())>0:\n",
    "                word[tag_index] = word[tag_index].lower().strip().strip(punctuation)\n",
    "                cleaned_sent.append(word)\n",
    "        cleaned_sents.append(cleaned_sent)\n",
    "    return cleaned_sents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_and_clean_corpus(tag_index, file_name='wikicorpus.txt'):\n",
    "    sents = read_data(file_name)\n",
    "    cleaned_tagged_sents = clean_sents(sents, tag_index)\n",
    "    return cleaned_tagged_sents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at lemmas of just nouns for our targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_tokenizer(s):\n",
    "    return s.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_word_count_index(sents, min_frequency):\n",
    "    text = [\" \".join(sent) for sent in sents]\n",
    "    vectorizer = CountVectorizer(ngram_range=(1,1), tokenizer=my_tokenizer, #use my own tokenizer so it doesn't split on -\n",
    "                               token_pattern='(?u)\\b\\S.*\\b',                #use my own token pattern to accept hyphenated words\n",
    "                               min_df=min_frequency)                        #ignore words that don't occur at least 10 times\n",
    "    \n",
    "    vectorizer.fit_transform(text)\n",
    "    return vectorizer\n",
    "\n",
    "def get_target_words_index(cleaned_tagged_sents, tag_index, min_frequency_target_words=50):\n",
    "    target_sents = [[word[tag_index] for word in sent\n",
    "                  if word[2].startswith('N')] for sent in cleaned_tagged_sents]\n",
    "    return get_word_count_index(target_sents, min_frequency_target_words)\n",
    "\n",
    "def remove_tags_and_infrequent_words(cleaned_tagged_sents, tag_index, min_frequency_context_words=20):\n",
    "    corpus_sents = [[word[tag_index] for word in sent] for sent in cleaned_tagged_sents] \n",
    "    vectorizer_corpus = get_word_count_index(corpus_sents, min_frequency_context_words)\n",
    "    corpus_words = set([corpus_sent for corpus_sent in chain.from_iterable(corpus_sents)])\n",
    "    words_to_filter = corpus_words.difference(vectorizer_corpus.vocabulary_.keys())\n",
    "    filtered_corpus_sents = [[word for word in sent if word not in words_to_filter] for sent in corpus_sents]\n",
    "    return filtered_corpus_sents, vectorizer_corpus\n",
    "\n",
    "def create_collocation_matrix_with_window(filtered_corpus_sents, vectorizer_corpus, vectorizer_target, window_size=2):\n",
    "    filtered_vocab_length =  len(vectorizer_corpus.vocabulary_)\n",
    "    rows = filtered_vocab_length\n",
    "    cols = filtered_vocab_length\n",
    "    target_words = set(vectorizer_target.vocabulary_.keys())\n",
    "    target_context_matrix = lil_matrix((rows, cols), dtype = np.int)\n",
    "    for context_sent in filtered_corpus_sents:\n",
    "        for target_index, target_word in enumerate(context_sent):\n",
    "            if(target_word in target_words):\n",
    "                #AFTER context\n",
    "                for i in xrange(1,window_size+1):\n",
    "                    if(target_index+i) >= len(context_sent):\n",
    "                        break #reached end of sentence!\n",
    "                    else:\n",
    "                        context_word = context_sent[target_index+i]\n",
    "                        target_context_matrix[vectorizer_corpus.vocabulary_.get(target_word),\n",
    "                                              vectorizer_corpus.vocabulary_.get(context_word)] += 1\n",
    "                #BEFORE context\n",
    "                for i in xrange(1,window_size+1):\n",
    "                    if(target_index-i) < 0:\n",
    "                        break #reached end of sentence!\n",
    "                    else:\n",
    "                        context_word = context_sent[target_index-i]\n",
    "                        target_context_matrix[vectorizer_corpus.vocabulary_.get(target_word),\n",
    "                                              vectorizer_corpus.vocabulary_.get(context_word)] += 1\n",
    "    return target_context_matrix\n",
    "\n",
    "def remove_empty_rows_columns(target_context_matrix, vectorizer_corpus):\n",
    "    print \"original shape: \", target_context_matrix.shape\n",
    "    # remove the rows        \n",
    "    target_row_sums = target_context_matrix.sum(axis = 1) #sum across the rows\n",
    "    target_word_index_no_zeros = []\n",
    "    target_vocab_no_zeros = []\n",
    "    dropped_target_words = []\n",
    "    dropped_context_words = []\n",
    "    original_word_index = vectorizer_corpus.get_feature_names()\n",
    "    for word_index in range(0,len(original_word_index)):\n",
    "        if target_row_sums[ word_index ] != 0:\n",
    "            target_word_index_no_zeros.append(word_index)\n",
    "            target_vocab_no_zeros.append(original_word_index[word_index])\n",
    "        else:\n",
    "            dropped_target_words.append(original_word_index[word_index])\n",
    "    target_context_matrix_no_zeros = target_context_matrix[target_word_index_no_zeros, :]\n",
    "\n",
    "    # remove the columns\n",
    "    context_row_sums = target_context_matrix.sum(axis = 0) #sum down the columns\n",
    "    context_word_index_no_zeros = []\n",
    "    context_vocab_no_zeros = []\n",
    "    for word_index in range(0,len(original_word_index)):\n",
    "        if context_row_sums[0,word_index] != 0:\n",
    "            context_word_index_no_zeros.append(word_index)\n",
    "            context_vocab_no_zeros.append(original_word_index[word_index])\n",
    "        else:\n",
    "            dropped_context_words.append(original_word_index[word_index])\n",
    "    target_context_matrix_no_zeros = target_context_matrix_no_zeros[:, context_word_index_no_zeros]\n",
    "    \n",
    "    return target_context_matrix_no_zeros, target_vocab_no_zeros, dropped_target_words, dropped_context_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_collocation_matrix(cleaned_sents, tag_index):\n",
    "    vectorizer_target = get_target_words_index(cleaned_sents, tag_index)\n",
    "    filtered_corpus_sents, vectorizer_corpus = remove_tags_and_infrequent_words(cleaned_sents, tag_index)\n",
    "    target_context_matrix = create_collocation_matrix_with_window(filtered_corpus_sents, vectorizer_corpus, vectorizer_target)\n",
    "    target_context_matrix, target_word_index, dropped_target_words, dropped_context_words = remove_empty_rows_columns(target_context_matrix, vectorizer_corpus)\n",
    "    if(len(dropped_target_words) | len(dropped_context_words)):\n",
    "        print \"Dropped \", len(dropped_target_words), \" target words\"\n",
    "        print \"Dropped \", len(dropped_context_words), \" context words\"\n",
    "    return target_context_matrix, target_word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get collocations\n",
    "#### stop at sentence boundary\n",
    "* T x C matrix, where T=target words (rows) and C=context words (columns)  \n",
    "* use lil_matrix because it is efficient for building sparse matrices  \n",
    "* Note: We technically don't need to build a full co-occurence matrix because our target words are just the top 50 nouns, but for flexibliity we will build a C x C matrix so that we can easily switch to a different set of target words later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove rows and columns with zero values (to avoid problems with division later)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate association measures\n",
    "#### <font color='red'>Important:</font> need to convert our co-occurence lil matrix to a csr matrix beore we do any mathemtical operations!\n",
    "1. $ PMI(target,context) = log\\frac{P(target,context)}{P(target) P(context)} $  \n",
    "  * $P(target,context) = \\frac{count(target,context)}{count(\\_\\_,\\_\\_)}$  \n",
    "  * $P(target) = \\frac{count(target,\\_\\_)}{count(\\_\\_,\\_\\_)}$  \n",
    "  * $P(context) = \\frac{count(context,\\_\\_)}{count(\\_\\_,\\_\\_)}$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def calculate_ppmi(target_context_matrix):\n",
    "    target_context_matrix = target_context_matrix.tocsr()\n",
    "    count_all = target_context_matrix.sum()\n",
    "    count_target = target_context_matrix.sum(axis = 1) #rows\n",
    "    count_context = target_context_matrix.sum(axis = 0) #columns\n",
    "\n",
    "    prob_target = count_target / count_all\n",
    "    prob_context = count_context / count_all\n",
    "\n",
    "    #prob target and context\n",
    "    pmi_target_context_matrix = target_context_matrix / count_all\n",
    "\n",
    "    #divide by prob target\n",
    "    pmi_target_context_matrix = pmi_target_context_matrix / prob_target\n",
    "\n",
    "    #divide by prob context\n",
    "    pmi_target_context_matrix = pmi_target_context_matrix / prob_context\n",
    "\n",
    "    #take log -- this will generate a divide by zero warning because we are taking log of 0\n",
    "    pmi_target_context_matrix = np.log(pmi_target_context_matrix)\n",
    "    #replace all the -inf with large negative numbers\n",
    "    #pmi_target_context_matrix - np.nan_to_num(pmi_target_context_matrix)\n",
    "    \n",
    "    ppmi_target_context_matrix = np.maximum(pmi_target_context_matrix, 0)\n",
    "    return ppmi_target_context_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positive PMI: convert all negative numbers to zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted jaccard:\n",
    "$j (word_1, word_2) = \\frac{\\sum{min(word_1[dim_i], word_2[dim_i]})}{\\sum{max(word_1[dim_i], word_2[dim_i])}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_similarities_for_word(target_context_matrix, target_word_index, word_of_interest, sim_measure='weighted_jaccard', top_k=20):\n",
    "    j_sims_list = []\n",
    "    target_word_to_index = dict(zip(target_word_index, list(range(0, len(target_word_index)))))\n",
    "\n",
    "    index_word_of_interest = target_word_to_index.get(word_of_interest)\n",
    "    for target_word in target_word_index:\n",
    "        if target_word != word_of_interest:\n",
    "            index_target_word = target_word_to_index.get(target_word)  \n",
    "            numerator = np.minimum(target_context_matrix[index_word_of_interest], target_context_matrix[index_target_word]).sum()\n",
    "            denominator = np.maximum(target_context_matrix[index_word_of_interest], target_context_matrix[index_target_word]).sum()\n",
    "            j_sims_list.append((target_word, (numerator/denominator)))\n",
    "    sorted_sims = sorted(j_sims_list, key=lambda sim: sim[1], reverse=True)\n",
    "    return sorted_sims[:top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_similarities_for_words(target_context_matrix, words_of_interest, top_k=20):\n",
    "    word_to_similarities = {}\n",
    "    for word_of_interest in words_of_interest:\n",
    "        word_to_similarities[word_of_interest] = get_similarities_for_word(target_context_matrix, word_of_interest, 20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in BLESS data set (tab-separated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_eval_data(file_name='BLESS_part.txt'):\n",
    "    with open(file_name, 'rb') as f:\n",
    "        bless_file = f.readlines()\n",
    "    bless_data = [line.split('\\t') for line in bless_file] # concept, class, relation, relatum\n",
    "    positive_pairs = [(data[0].split('-')[0], data[3].split('-')[0]) for data in bless_data if data[2]== \"coord\" or data[2]==\"hyper\"]\n",
    "    negative_pairs = [(data[0].split('-')[0], data[3].split('-')[0]) for data in bless_data if data[2]== \"mero\" or data[2]==\"random-n\"]\n",
    "    return positive_pairs, negative_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy @1 and @5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_scores(words_of_interest, target_context_matrix, target_word_index):\n",
    "    words_to_similarities = {}\n",
    "    words_to_scores = {}\n",
    "    accuracy_level = 5\n",
    "    \n",
    "    positive_pairs, _ = get_eval_data()\n",
    "    \n",
    "    for word_of_interest in words_of_interest:\n",
    "        similarities = get_similarities_for_word(target_context_matrix, target_word_index, word_of_interest)\n",
    "        words_to_similarities[word_of_interest] = similarities\n",
    "        accuracy_1 = 0\n",
    "        accuracy_5 = 0\n",
    "        for i in range(0,accuracy_level):\n",
    "            if (word_of_interest,similarities[i][0]) in positive_pairs:\n",
    "                if i==0:\n",
    "                    accuracy_1 = 1\n",
    "                accuracy_5 += 1    \n",
    "        accuracy_5 /= accuracy_level\n",
    "        words_to_scores[word_of_interest] = (accuracy_1, accuracy_5)\n",
    "    scores_array = np.asarray(words_to_scores.values())\n",
    "    print \"Average scores: \", np.mean(scores_array,axis=0)\n",
    "    print words_to_similarities\n",
    "    return np.mean(scores_array,axis=0), words_to_similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's put it all together now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_dist_space(assoc_measure='ppmi', tag_index=1): #lemma\n",
    "    cleaned_sents = load_and_clean_corpus(tag_index)\n",
    "    target_context_matrix, target_word_index = create_collocation_matrix(cleaned_sents, tag_index)\n",
    "    target_context_matrix_ppmi = calculate_ppmi(target_context_matrix)\n",
    "    return target_context_matrix_ppmi, target_word_index\n",
    "    \n",
    "def evaluate_similarities(words_of_interest, target_context_matrix, target_word_index):\n",
    "    words_to_scores,words_to_similarities  = get_scores(words_of_interest, target_context_matrix, target_word_index)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average scores:  [ 0.41666667  0.28333333]\n",
      "{'horse': [(u'cavalry', 0.089168351009227265), (u'breed', 0.088934040702232681), (u'ride', 0.085833037620169389), (u'dog', 0.084762836465707564), (u'infantry', 0.078155757924123673), (u'cat', 0.075974609160722642), (u'cattle', 0.073298121335489547), (u'guard', 0.071783858820123683), (u'hunting', 0.071090585340690668), (u'camel', 0.070678610212304094), (u'animal', 0.070479364506001813), (u'foot', 0.069525471459616614), (u'soldier', 0.067807764193132961), (u'arm', 0.067557975650351418), (u'mount', 0.065356432752122412), (u'car', 0.064230313858709601), (u'kill', 0.064194697134183867), (u'shoot', 0.064169795500626334), (u'gun', 0.064138470248621834), (u'goat', 0.064000095875275012)], 'bomb': [(u'bomber', 0.11652294813541221), (u'bombing', 0.1152279231539908), (u'missile', 0.096065285876594936), (u'weapon', 0.094589898534941305), (u'aircraft', 0.09456271585578041), (u'rocket', 0.092226166852020411), (u'fighter', 0.081785994036849066), (u'combat', 0.079683659143066923), (u'fly', 0.079037386835713389), (u'gun', 0.078493960262631113), (u'nuclear', 0.07838710888612252), (u'crew', 0.078128857440945457), (u'explosive', 0.077755893517933633), (u'squadron', 0.077233845850688918), (u'allied', 0.076942902832142793), (u'target', 0.075994756709511682), (u'sink', 0.075632913875160593), (u'shell', 0.07363468235064996), (u'mission', 0.07298647803865356), (u'battleship', 0.071808845746903618)], 'car': [(u'vehicle', 0.12734611716439195), (u'engine', 0.12042473948695108), (u'racing', 0.10393699057654548), (u'train', 0.093848717746600196), (u'drive', 0.092577449533800663), (u'aircraft', 0.09123021784800367), (u'race', 0.091063555446218858), (u'passenger', 0.090319380270192665), (u'truck', 0.089353932364232641), (u'bus', 0.087571782409128732), (u'road', 0.086710739119147034), (u'motor', 0.085921761017049528), (u'company', 0.083394415453855952), (u'ship', 0.083111103601867062), (u'track', 0.08043520676585747), (u'diesel', 0.080418704290977805), (u'driver', 0.079694753303258301), (u'boat', 0.07965532688906779), (u'run', 0.079364232625504058), (u'design', 0.07847484570289999)], 'hospital': [(u'campus', 0.087005518588646427), (u'medical', 0.085984932213407), (u'university', 0.079912971957132733), (u'care', 0.075270701225401734), (u'college', 0.075024407945014415), (u'facility', 0.071719755172931915), (u'michigan', 0.070281544499782661), (u'patient', 0.069721960735770944), (u'resident', 0.069483517441157924), (u'london', 0.069173242714870839), (u'health', 0.068965084635889976), (u'institute', 0.068781453943354207), (u'airport', 0.068545464350134255), (u'centre', 0.067332615325689182), (u'theatre', 0.065966282014869038), (u'hotel', 0.065739852073837812), (u'metropolitan', 0.065359910892856712), (u'headquarters', 0.064354779266065337), (u'railway', 0.063897841387055399), (u'district', 0.063586516040631647)], 'hotel': [(u'restaurant', 0.10612027447028957), (u'tourist', 0.08470481820919866), (u'downtown', 0.078342757509587996), (u'apartment', 0.07785271327344187), (u'tourism', 0.076721623807458353), (u'street', 0.0762644930439315), (u'beach', 0.075872690986747426), (u'park', 0.075792867911697911), (u'resort', 0.075318453799352855), (u'palace', 0.074647767188320571), (u'room', 0.073706212600506568), (u'village', 0.072770369187739561), (u'building', 0.072485141967530781), (u'headquarters', 0.071897513047983155), (u'museum', 0.070978585494054114), (u'neighborhood', 0.070969421805088917), (u'facility', 0.070446966652092102), (u'suburb', 0.069775980511913027), (u'campus', 0.069179314875213657), (u'san', 0.068870562616912503)], 'fox': [(u'coyote', 0.077264344032174165), (u'cbs', 0.074957680452214737), (u'abc', 0.071345674530159026), (u'nbc', 0.068838167626096791), (u'wolf', 0.064589668157802052), (u'broadcast', 0.06175517033464846), (u'dog', 0.061545974618915499), (u'interview', 0.061237124297388694), (u'tv', 0.060896336113484488), (u'television', 0.05896428127413416), (u'hunt', 0.058811808652486648), (u'entertainment', 0.058153714223707215), (u'wild', 0.05740242888972568), (u'sports', 0.055625076452091055), (u'movie', 0.055615795132864479), (u'drama', 0.055477593864005283), (u'documentary', 0.055199737792724524), (u'news', 0.054540880869125875), (u'mr', 0.054492204267551442), (u'coverage', 0.05442828524117134)], 'gun': [(u'tank', 0.1315711917538461), (u'weapon', 0.12998446699999858), (u'cannon', 0.12224471179881592), (u'artillery', 0.11951453629861661), (u'armour', 0.11252813798435352), (u'vehicle', 0.10710565224778151), (u'missile', 0.10678342038280722), (u'fire', 0.10590369799234765), (u'aircraft', 0.10581161971883538), (u'ship', 0.10512243643020951), (u'mount', 0.10127677179771979), (u'cruiser', 0.098114901155283499), (u'arm', 0.097495819170847708), (u'destroyer', 0.096257134408754663), (u'rifle', 0.095931141919677082), (u'turret', 0.093977583911555945), (u'battleship', 0.090997114116650901), (u'torpedo', 0.090009880576038703), (u'enemy', 0.08790618422000479), (u'battery', 0.086674458244037106)], 'guitar': [(u'bass', 0.17599691730932701), (u'instrument', 0.13052535296184237), (u'drum', 0.11855808603021481), (u'jazz', 0.11139604594637428), (u'vocal', 0.10610699915964827), (u'guitarist', 0.10284326599118004), (u'solo', 0.10119957357657372), (u'piano', 0.10003643944654428), (u'musician', 0.095781969200312497), (u'recording', 0.095571211385613253), (u'string', 0.094600256311725472), (u'keyboard', 0.09362764107553638), (u'cello', 0.093577378016409585), (u'bassist', 0.09337148112479228), (u'tune', 0.093061602422612508), (u'banjo', 0.092387505151796612), (u'blues', 0.092192419009310217), (u'band', 0.089834221485571839), (u'sound', 0.087665935156499841), (u'violin', 0.085522796065952103)], 'bus': [(u'rail', 0.12267856727598825), (u'passenger', 0.10105314869006585), (u'train', 0.097690551426920472), (u'vehicle', 0.096858453860681828), (u'route', 0.094265456829987285), (u'cable', 0.094031136264425982), (u'transport', 0.093966499299305034), (u'railway', 0.093499882079369948), (u'traffic', 0.0929867153256516), (u'network', 0.091916037915315843), (u'chip', 0.090079725948244024), (u'station', 0.089147874375995861), (u'airport', 0.088443043035377686), (u'processor', 0.087643081788115001), (u'car', 0.087571782409128732), (u'interface', 0.087022108834032164), (u'terminal', 0.086508225663892366), (u'device', 0.085439128673234485), (u'transit', 0.084940634772120216), (u'transportation', 0.084184082634999186)], 'table': [(u'string', 0.078627595565433889), (u'simple', 0.076648458955474535), (u'datum', 0.076639568227058599), (u'round', 0.075428652003315069), (u'ball', 0.075384405415873448), (u'object', 0.074598208785378317), (u'sequence', 0.074107317241757453), (u'multiple', 0.07355668795364867), (u'list', 0.073256764054179815), (u'map', 0.073040100080505807), (u'symbol', 0.071555557932515099), (u'column', 0.071500777114267555), (u'element', 0.070970728416271389), (u'room', 0.070711389333617203), (u'pair', 0.070334767022553674), (u'bet', 0.069828798567983075), (u'category', 0.06979737769254965), (u'equivalent', 0.069497070654748999), (u'length', 0.06919565987126608), (u'combination', 0.069097716866832984)], 'piano': [(u'violin', 0.13194815161613963), (u'concerto', 0.12971380950790878), (u'sonata', 0.1290255879134517), (u'bass', 0.10685798940768569), (u'solo', 0.10121316507058122), (u'orchestra', 0.10046956422647491), (u'guitar', 0.10003643944654428), (u'cello', 0.094568691041170258), (u'jazz', 0.094567954727962325), (u'beethoven', 0.093391954717678305), (u'debussy', 0.090129288138337024), (u'melody', 0.089943744557598085), (u'symphony', 0.089884610071352122), (u'composer', 0.088506249131695733), (u'quartet', 0.087640342892467471), (u'banjo', 0.087379476133194675), (u'ensemble', 0.081348038786399068), (u'string', 0.080881414870255908), (u'drum', 0.079609104929036625), (u'clarinet', 0.078581852239144531)], 'bowl': [(u'super', 0.17192783632476452), (u'playoff', 0.11298618424846472), (u'nfl', 0.10186867276390475), (u'championship', 0.095686061199817318), (u'bears', 0.086620152742392514), (u'champion', 0.08596451700050449), (u'cowboys', 0.083715117062554842), (u'football', 0.080101886190814472), (u'winner', 0.079320552333307592), (u'cup', 0.079075112615029558), (u'tournament', 0.079007482369138474), (u'stadium', 0.077956671884116729), (u'coach', 0.077118127249663188), (u'afl', 0.076668021328062594), (u'browns', 0.076217355238114154), (u'season', 0.073618731326114503), (u'cleveland', 0.072615284754546164), (u'dallas', 0.072246060327627773), (u'quarterback', 0.072245796678764199), (u'afc', 0.07130737637527014)]}\n",
      "CPU times: user 15 s, sys: 718 ms, total: 15.7 s\n",
      "Wall time: 16.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "words_of_interest = ['car', 'bus', 'hospital', 'hotel', 'gun', 'bomb', 'horse', 'fox', 'table', 'bowl', 'guitar', 'piano']\n",
    "target_context_matrix, target_word_index = create_dist_space()\n",
    "evaluate_similarities(words_of_interest, target_context_matrix, target_word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
