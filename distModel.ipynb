{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from itertools import chain \n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.collocations import *\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from scipy.sparse import lil_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from string import punctuation\n",
    "\n",
    "from __future__ import division\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Watermark - tool to help with reproducibility:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%install_ext https://raw.githubusercontent.com/rasbt/watermark/master/watermark/watermark.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last updated: Tue Jul 12 2016 18:56:11 CDT\n",
      "\n",
      "CPython 2.7.11\n",
      "IPython 4.0.3\n",
      "\n",
      "nltk 3.0.3\n",
      "numpy 1.10.1\n",
      "scipy 0.17.0\n",
      "\n",
      "compiler   : GCC 4.2.1 (Apple Inc. build 5577)\n",
      "system     : Darwin\n",
      "release    : 15.5.0\n",
      "machine    : x86_64\n",
      "processor  : i386\n",
      "CPU cores  : 4\n",
      "interpreter: 64bit\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark -n -t -z -u -m -v -p nltk,numpy,scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[u'Anarchism', u'Anarchism', u'NNP', u'I-NP', u'O', u'N'], [u'.', u'.', u'.', u'O', u'O', u'.\\n']], [[u'Anarchism', u'Anarchism', u'NNP', u'I-NP', u'O', u'N'], [u'is', u'be', u'VBZ', u'I-VP', u'O', u'(S[dcl]\\\\NP)/NP'], [u'a', u'a', u'DT', u'I-NP', u'O', u'NP[nb]/N'], [u'political', u'political', u'JJ', u'I-NP', u'O', u'N/N'], [u'philosophy', u'philosophy', u'NN', u'I-NP', u'O', u'N'], [u'encompassing', u'encompass', u'VBG', u'I-VP', u'O', u'(S[ng]\\\\NP)/NP'], [u'theories', u'theory', u'NNS', u'I-NP', u'O', u'N'], [u'and', u'and', u'CC', u'I-NP', u'O', u'conj'], [u'attitudes', u'attitude', u'NNS', u'I-NP', u'O', u'N'], [u'which', u'which', u'WDT', u'B-NP', u'O', u'(NP\\\\NP)/(S[dcl]\\\\NP)'], [u'consider', u'consider', u'VBP', u'I-VP', u'O', u'((S[dcl]\\\\NP)/(S[to]\\\\NP))/NP'], [u'the', u'the', u'DT', u'I-NP', u'O', u'NP[nb]/N'], [u'state', u'state', u'NN', u'I-NP', u'O', u'N'], [u'to', u'to', u'TO', u'I-VP', u'O', u'(S[to]\\\\NP)/(S[b]\\\\NP)'], [u'be', u'be', u'VB', u'I-VP', u'O', u'(S[b]\\\\NP)/(S[adj]\\\\NP)'], [u'unnecessary', u'unnecessary', u'JJ', u'I-ADJP', u'O', u'S[adj]\\\\NP'], [u',', u',', u',', u'I-ADJP', u'O', u','], [u'harmful', u'harmful', u'JJ', u'I-ADJP', u'O', u'S[adj]\\\\NP'], [u',', u',', u',', u'I-ADJP', u'O', u','], [u'and/', u'and/', u'JJ', u'I-ADJP', u'O', u'S[adj]\\\\NP'], [u'or', u'or', u'CC', u'I-ADJP', u'O', u'conj'], [u'undesirable', u'undesirable', u'JJ', u'I-ADJP', u'O', u'S[adj]\\\\NP'], [u'.', u'.', u'.', u'O', u'O', u'.\\n']], [[u'Specific', u'Specific', u'NNP', u'I-NP', u'O', u'N/N'], [u'anarchists', u'anarchist', u'NNS', u'I-NP', u'O', u'N'], [u'may', u'may', u'MD', u'I-VP', u'O', u'(S[dcl]\\\\NP)/(S[b]\\\\NP)'], [u'have', u'have', u'VB', u'I-VP', u'O', u'(S[b]\\\\NP)/NP'], [u'additional', u'additional', u'JJ', u'I-NP', u'O', u'N/N'], [u'criteria', u'criterion', u'NNS', u'I-NP', u'O', u'N'], [u'for', u'for', u'IN', u'I-PP', u'O', u'(NP\\\\NP)/NP'], [u'what', u'what', u'WP', u'I-NP', u'O', u'NP/(S[dcl]\\\\NP)'], [u'constitutes', u'constitute', u'VBZ', u'I-VP', u'O', u'(S[dcl]\\\\NP)/NP'], [u'anarchism', u'anarchism', u'NN', u'I-NP', u'O', u'N'], [u',', u',', u',', u'O', u'O', u','], [u'and', u'and', u'CC', u'O', u'O', u'conj'], [u'they', u'they', u'PRP', u'I-NP', u'O', u'NP'], [u'often', u'often', u'RB', u'I-ADVP', u'O', u'(S\\\\NP)/(S\\\\NP)'], [u'disagree', u'disagree', u'VBP', u'I-VP', u'O', u'(S[dcl]\\\\NP)/PP'], [u'with', u'with', u'IN', u'I-PP', u'O', u'PP/NP'], [u'each', u'each', u'DT', u'I-NP', u'O', u'NP[nb]/N'], [u'other', u'other', u'NN', u'I-NP', u'O', u'N'], [u'on', u'on', u'IN', u'I-PP', u'O', u'((S\\\\NP)\\\\(S\\\\NP))/NP'], [u'what', u'what', u'WP', u'I-NP', u'O', u'NP/(S[dcl]/NP)'], [u'these', u'these', u'DT', u'B-NP', u'O', u'NP[nb]/N'], [u'criteria', u'criterion', u'NNS', u'I-NP', u'O', u'N'], [u'are', u'be', u'VBP', u'I-VP', u'O', u'(S[dcl]\\\\NP)/NP'], [u'.', u'.', u'.', u'O', u'O', u'.\\n']]]\n",
      "CPU times: user 35.2 s, sys: 30.8 s, total: 1min 6s\n",
      "Wall time: 1min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "file_name = '/Users/elisa/Documents/CompLing/compSemantics/HW3/wikicorpus.txt'\n",
    "sents = []\n",
    "with open(file_name, 'rb') as f:\n",
    "    for line in f:\n",
    "        if line.startswith(\"<c> \"):\n",
    "            line = line.decode('cp1252') #convert from Windows Latin-1 encoding to avoid unicode issues\n",
    "            tagged_words = line.split(\" \")[1:] #skip the <c>\n",
    "            sents.append([tagged_word.split('|') for tagged_word in tagged_words])\n",
    "print sents[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up the data\n",
    "#### Remove stop words, 1 char words, punctuation, then lowercase and strip punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[u'Anarchism', u'anarchism', u'NNP', u'I-NP', u'O', u'N']], [[u'Anarchism', u'anarchism', u'NNP', u'I-NP', u'O', u'N'], [u'political', u'political', u'JJ', u'I-NP', u'O', u'N/N'], [u'philosophy', u'philosophy', u'NN', u'I-NP', u'O', u'N'], [u'encompassing', u'encompass', u'VBG', u'I-VP', u'O', u'(S[ng]\\\\NP)/NP'], [u'theories', u'theory', u'NNS', u'I-NP', u'O', u'N'], [u'attitudes', u'attitude', u'NNS', u'I-NP', u'O', u'N'], [u'consider', u'consider', u'VBP', u'I-VP', u'O', u'((S[dcl]\\\\NP)/(S[to]\\\\NP))/NP'], [u'state', u'state', u'NN', u'I-NP', u'O', u'N'], [u'unnecessary', u'unnecessary', u'JJ', u'I-ADJP', u'O', u'S[adj]\\\\NP'], [u'harmful', u'harmful', u'JJ', u'I-ADJP', u'O', u'S[adj]\\\\NP'], [u'and/', u'and', u'JJ', u'I-ADJP', u'O', u'S[adj]\\\\NP'], [u'undesirable', u'undesirable', u'JJ', u'I-ADJP', u'O', u'S[adj]\\\\NP']], [[u'Specific', u'specific', u'NNP', u'I-NP', u'O', u'N/N'], [u'anarchists', u'anarchist', u'NNS', u'I-NP', u'O', u'N'], [u'may', u'may', u'MD', u'I-VP', u'O', u'(S[dcl]\\\\NP)/(S[b]\\\\NP)'], [u'additional', u'additional', u'JJ', u'I-NP', u'O', u'N/N'], [u'criteria', u'criterion', u'NNS', u'I-NP', u'O', u'N'], [u'constitutes', u'constitute', u'VBZ', u'I-VP', u'O', u'(S[dcl]\\\\NP)/NP'], [u'anarchism', u'anarchism', u'NN', u'I-NP', u'O', u'N'], [u'often', u'often', u'RB', u'I-ADVP', u'O', u'(S\\\\NP)/(S\\\\NP)'], [u'disagree', u'disagree', u'VBP', u'I-VP', u'O', u'(S[dcl]\\\\NP)/PP'], [u'criteria', u'criterion', u'NNS', u'I-NP', u'O', u'N']]]\n",
      "CPU times: user 23.2 s, sys: 32.4 s, total: 55.6 s\n",
      "Wall time: 1min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "stopwords_set = set(stopwords.words('english'))\n",
    "punctuation_set = set(punctuation)\n",
    "tag_index = 1 #lemma\n",
    "cleaned_sents = []\n",
    "for sent in sents:\n",
    "    cleaned_sent = []\n",
    "    for word in sent:\n",
    "        if word[0].lower() not in stopwords_set and word[0] not in punctuation_set and len(word[0].strip())>1 :\n",
    "            word[tag_index] = word[tag_index].lower().strip(punctuation)\n",
    "            cleaned_sent.append(word)\n",
    "    cleaned_sents.append(cleaned_sent)\n",
    "print cleaned_sents[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at lemmas of just nouns for our targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'anarchism'], [u'anarchism', u'philosophy', u'theory', u'attitude', u'state'], [u'specific', u'anarchist', u'criterion', u'anarchism', u'criterion']]\n"
     ]
    }
   ],
   "source": [
    "target_sents = [[word[tag_index] for word in sent\n",
    "                  if word[2].startswith('N')] for sent in cleaned_sents] \n",
    "print target_sents[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get noun lemmas that occur at least 50 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target_text = [\" \".join(target_sent) for target_sent in target_sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'time', 14036L), (u'year', 13207L), (u'system', 9913L), (u'city', 9647L), (u'number', 9520L), (u'world', 9081L), (u'state', 8448L), (u'part', 7911L), (u'example', 7226L), (u'century', 7143L)]\n",
      "CPU times: user 6.16 s, sys: 1.47 s, total: 7.63 s\n",
      "Wall time: 8.79 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def my_tokenizer(s):\n",
    "    return s.split()\n",
    "v_target = CountVectorizer(ngram_range=(1,1), token_pattern='(?u)\\b\\S.*\\b', tokenizer=my_tokenizer, min_df=50)\n",
    "unigram_matrix = v_target.fit_transform(target_text)\n",
    "#sort\n",
    "features_count = unigram_matrix.sum(axis=0).tolist()[0]\n",
    "features_names = v_target.get_feature_names()\n",
    "sorted_counts = sorted(zip(features_names, features_count), key=lambda count: count[1], reverse=True)\n",
    "print len(v_targetget.vocabulary_.keys())\n",
    "print sorted_counts[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How big is our  vocabulary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original vocabulary length: 206121\n"
     ]
    }
   ],
   "source": [
    "corpus_sents = [[word[tag_index] for word in sent] for sent in cleaned_sents] \n",
    "corpus_words = set([corpus_sent for corpus_sent in chain.from_iterable(corpus_sents)])\n",
    "print \"Original vocabulary length:\", len(corpus_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get rid of infrequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered vocabulary length: 18008\n",
      "CPU times: user 10.1 s, sys: 8.14 s, total: 18.2 s\n",
      "Wall time: 21.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "min_frequency_target_words = 20\n",
    "corpus_text = [\" \".join(corpus_sent) for corpus_sent in corpus_sents]\n",
    "v_corpus = CountVectorizer(ngram_range=(1,1), tokenizer=my_tokenizer, #use my own tokenizer so it doesn't split on -\n",
    "                           token_pattern='(?u)\\b\\S.*\\b',              #use my own token pattern to accept hyphenated words\n",
    "                           min_df=min_frequency_target_words)         #ignore words that don't occur at least x times\n",
    "corpus_matrix = v_corpus.fit_transform(corpus_text)\n",
    "corpus_count = corpus_matrix.sum(axis=0).tolist()[0]\n",
    "filtered_vocab_length =  len(v_corpus.vocabulary_)\n",
    "print \"Filtered vocabulary length:\", filtered_vocab_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'anarchism'], [u'anarchism', u'political', u'philosophy', u'encompass', u'theory', u'attitude', u'consider', u'state', u'unnecessary', u'harmful', u'and', u'undesirable'], [u'specific', u'anarchist', u'may', u'additional', u'criterion', u'constitute', u'anarchism', u'often', u'disagree', u'criterion']]\n",
      "CPU times: user 4.1 s, sys: 8.13 s, total: 12.2 s\n",
      "Wall time: 16.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "words_to_filter = corpus_words.difference(v_corpus.vocabulary_.keys())\n",
    "filtered_corpus_sents = [[word for word in sent if word not in words_to_filter] for sent in corpus_sents]\n",
    "print filtered_corpus_sents[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get collocations\n",
    "#### stop at sentence boundary\n",
    "* T x C matrix, where T=target words (rows) and C=context words (columns)  \n",
    "* use lil_matrix because it is efficient for building sparse matrices  \n",
    "* Note: We technically don't need to build a full co-occurence matrix because our target words are just the top 50 nouns, but for flexibliity we will build a C x C matrix so that we can easily switch to a different set of target words later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18008, 18008)\n",
      "CPU times: user 3min 47s, sys: 21.3 s, total: 4min 8s\n",
      "Wall time: 4min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "window_size = 2\n",
    "rows = filtered_vocab_length\n",
    "cols = filtered_vocab_length\n",
    "target_words = set(v_target.vocabulary_.keys())\n",
    "target_context_matrix = lil_matrix((rows, cols), dtype = np.int)\n",
    "for context_sent in filtered_corpus_sents:\n",
    "    for target_index, target_word in enumerate(context_sent):\n",
    "        if(target_word in target_words):\n",
    "            #AFTER context\n",
    "            for i in xrange(1,window_size+1):\n",
    "                if(target_index+i) >= len(context_sent):\n",
    "                    break #reached end of sentence!\n",
    "                else:\n",
    "                    context_word = context_sent[target_index+i]\n",
    "                    target_context_matrix[v_corpus.vocabulary_.get(target_word),\n",
    "                                          v_corpus.vocabulary_.get(context_word)] += 1\n",
    "            #BEFORE context\n",
    "            for i in xrange(1,window_size+1):\n",
    "                if(target_index-i) < 0:\n",
    "                    break #reached end of sentence!\n",
    "                else:\n",
    "                    context_word = context_sent[target_index-i]\n",
    "                    target_context_matrix[v_corpus.vocabulary_.get(target_word),\n",
    "                                          v_corpus.vocabulary_.get(context_word)] += 1\n",
    "print target_context_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check: print co-occurence of time with year, \n",
    "should be 213"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "213\n"
     ]
    }
   ],
   "source": [
    "print target_context_matrix[(v_corpus.vocabulary_.get('time'),v_corpus.vocabulary_.get('year'))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove rows and columns with zero values (to avoid problems with division later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6704, 18008)\n",
      "(6704, 18008)\n"
     ]
    }
   ],
   "source": [
    "# remove the rows        \n",
    "target_row_sums = target_context_matrix.sum(axis = 1) #sum across the rows\n",
    "target_word_index_no_zeros = []\n",
    "target_vocab_no_zeros = []\n",
    "original_word_index = v_corpus.get_feature_names()\n",
    "for word_index in range(0,len(original_word_index)):\n",
    "    if target_row_sums[ word_index ] != 0:\n",
    "        target_word_index_no_zeros.append(word_index)\n",
    "        target_vocab_no_zeros.append(original_word_index[word_index])\n",
    "target_context_matrix_no_zeros = target_context_matrix[target_word_index_no_zeros, :]\n",
    "print target_context_matrix_no_zeros.shape\n",
    "\n",
    "# remove the columns\n",
    "context_row_sums = target_context_matrix.sum(axis = 0) #sum down the columns\n",
    "context_word_index_no_zeros = []\n",
    "context_vocab_no_zeros = []\n",
    "for word_index in range(0,len(original_word_index)):\n",
    "    if context_row_sums[ 0,word_index ] != 0:\n",
    "        context_word_index_no_zeros.append(word_index)\n",
    "        context_vocab_no_zeros.append(original_word_index[word_index])\n",
    "target_context_matrix_no_zeros = target_context_matrix_no_zeros[:, context_word_index_no_zeros]\n",
    "print target_context_matrix_no_zeros.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate association measures\n",
    "#### <font color='red'>Important:</font> need to convert our co-occurence lil matrix to a csr matrix beore we do any mathemtical operations!\n",
    "1. $ PMI(target,context) = log\\frac{P(target,context)}{P(target) P(context)} $  \n",
    "  * $P(target,context) = \\frac{count(target,context)}{count(\\_\\_,\\_\\_)}$  \n",
    "  * $P(target) = \\frac{count(target,\\_\\_)}{count(\\_\\_,\\_\\_)}$  \n",
    "  * $P(context) = \\frac{count(context,\\_\\_)}{count(\\_\\_,\\_\\_)}$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.38 s, sys: 4.97 s, total: 10.3 s\n",
      "Wall time: 12.6 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elisa/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:19: RuntimeWarning: divide by zero encountered in log\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "target_context_matrix = target_context_matrix_no_zeros.tocsr()\n",
    "count_all = target_context_matrix.sum()\n",
    "count_target = target_context_matrix.sum(axis = 1) #rows\n",
    "count_context = target_context_matrix.sum(axis = 0) #columns\n",
    "\n",
    "prob_target = count_target / count_all\n",
    "prob_context = count_context / count_all\n",
    "\n",
    "#prob target and context\n",
    "pmi_target_context_matrix = target_context_matrix / count_all\n",
    "\n",
    "#divide by prob target\n",
    "pmi_target_context_matrix = pmi_target_context_matrix / prob_target\n",
    "\n",
    "#divide by prob context\n",
    "pmi_target_context_matrix = pmi_target_context_matrix / prob_context\n",
    "\n",
    "#take log -- this will generate a divide by zero warning because we are taking log of 0\n",
    "pmi_target_context_matrix = np.log(pmi_target_context_matrix)\n",
    "#replace all the -inf with large negative numbers\n",
    "#pmi_target_context_matrix - np.nan_to_num(pmi_target_context_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-inf -inf -inf ..., -inf -inf -inf]\n",
      " [-inf -inf -inf ..., -inf -inf -inf]\n",
      " [-inf -inf -inf ..., -inf -inf -inf]\n",
      " ..., \n",
      " [-inf -inf -inf ..., -inf -inf -inf]\n",
      " [-inf -inf -inf ..., -inf -inf -inf]\n",
      " [-inf -inf -inf ..., -inf -inf -inf]]\n"
     ]
    }
   ],
   "source": [
    "print pmi_target_context_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print PMI for time and year. Should be 0.6495"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time\n",
      "year\n",
      "0.64944562357\n"
     ]
    }
   ],
   "source": [
    "print target_vocab_no_zeros[6185]\n",
    "print context_vocab_no_zeros[17914]\n",
    "print pmi_target_context_matrix[(6185,17914)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positive PMI: convert all negative numbers to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "ppmi_target_context_matrix = np.maximum(pmi_target_context_matrix, 0)\n",
    "print ppmi_target_context_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted jaccard:\n",
    "$j (word_1, word_2) = \\frac{\\sum{min(word_1[dim_i], word_2[dim_i]})}{\\sum{max(word_1[dim_i], word_2[dim_i])}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_similarities_for_word(target_context_matrix, word_of_interest, top_k):\n",
    "    j_sims_list = []\n",
    "    target_word_to_index = dict(zip(target_vocab_no_zeros, list(range(0, len(target_vocab_no_zeros)))))\n",
    "\n",
    "    index_word_of_interest = target_word_to_index.get(word_of_interest)\n",
    "    for target_word in target_vocab_no_zeros:\n",
    "        if target_word != word_of_interest:\n",
    "            index_target_word = target_word_to_index.get(target_word)  \n",
    "            numerator = np.minimum(ppmi_target_context_matrix[index_word_of_interest], ppmi_target_context_matrix[index_target_word]).sum()\n",
    "            denominator = np.maximum(ppmi_target_context_matrix[index_word_of_interest], ppmi_target_context_matrix[index_target_word]).sum()\n",
    "            j_sims_list.append((target_word, (numerator/denominator)))\n",
    "    sorted_sims = sorted(j_sims_list, key=lambda sim: sim[1], reverse=True)\n",
    "    return sorted_sims[:top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14 s, sys: 684 ms, total: 14.7 s\n",
      "Wall time: 15 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "similarities_dict = {}\n",
    "words_of_interest = ['car', 'bus', 'hospital', 'hotel', 'gun', 'bomb', 'horse', 'fox', 'table', 'bowl', 'guitar', 'piano']\n",
    "for word_of_interest in words_of_interest:\n",
    "    similarities_dict[word_of_interest] = get_similarities_for_word(ppmi_target_context_matrix, word_of_interest, 20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in BLESS data set (tab-separated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_name = '/Users/elisa/Documents/CompLing/compSemantics/HW3/BLESS_part.txt'\n",
    "with open(file_name, 'rb') as f:\n",
    "    bless_file = f.readlines()\n",
    "bless_data = [line.split('\\t') for line in bless_file] # concept, class, relation, relatum\n",
    "positive_pairs = [(data[0].split('-')[0], data[3].split('-')[0]) for data in bless_data if data[2]== \"coord\" or data[2]==\"hyper\"]\n",
    "negative_pairs = [(data[0].split('-')[0], data[3].split('-')[0]) for data in bless_data if data[2]== \"mero\" or data[2]==\"random-n\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy @1 and @5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scores_dict = {}\n",
    "accuracy_level = 5\n",
    "for word_of_interest in words_of_interest:\n",
    "    accuracy_1 = 0\n",
    "    accuracy_5 = 0\n",
    "    for i in range(0,accuracy_level):\n",
    "        if (word_of_interest,similarities_dict[word_of_interest][i][0]) in positive_pairs:\n",
    "            if i==0:\n",
    "                accuracy_1 = 1\n",
    "            accuracy_5 += 1    \n",
    "    accuracy_5 /= accuracy_level\n",
    "    scores_dict[word_of_interest] = (accuracy_1, accuracy_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average scores:  [ 0.25        0.28333333]\n"
     ]
    }
   ],
   "source": [
    "scores_array = np.asarray(scores_dict.values())\n",
    "print \"Average scores: \", np.mean(scores_array,axis=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
