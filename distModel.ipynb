{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from itertools import chain \n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "from nltk.corpus import stopwords\n",
    "from scipy.sparse import lil_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from string import punctuation\n",
    "\n",
    "from __future__ import division\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Watermark - tool to help with reproducibility:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%install_ext https://raw.githubusercontent.com/rasbt/watermark/master/watermark/watermark.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last updated: Fri Aug 12 2016 12:30:04 CDT\n",
      "\n",
      "CPython 2.7.11\n",
      "IPython 4.0.3\n",
      "\n",
      "nltk 3.0.3\n",
      "numpy 1.10.1\n",
      "scipy 0.17.0\n",
      "\n",
      "compiler   : GCC 4.2.1 (Apple Inc. build 5577)\n",
      "system     : Darwin\n",
      "release    : 15.6.0\n",
      "machine    : x86_64\n",
      "processor  : i386\n",
      "CPU cores  : 4\n",
      "interpreter: 64bit\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark -n -t -z -u -m -v -p nltk,numpy,scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_data(file_name):\n",
    "    sents = []\n",
    "    with open(file_name, 'rb') as f:\n",
    "        for line in f:\n",
    "            if line.startswith(\"<c> \"):\n",
    "                line = line.decode('cp1252') #convert from Windows Latin-1 encoding to avoid unicode issues\n",
    "                tagged_words = line.split(\" \")[1:] #skip the <c>\n",
    "                sents.append([tagged_word.split('|') for tagged_word in tagged_words])\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up the data\n",
    "#### Remove stop words, punctuation, empty strings, then lowercase and strip punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def clean_sents(sents, tag_index):\n",
    "    stopwords_set = set(stopwords.words('english'))\n",
    "    punctuation_set = set(punctuation)\n",
    "    cleaned_sents = []\n",
    "    for sent in sents:\n",
    "        cleaned_sent = []\n",
    "        for word in sent:\n",
    "            if word[1].lower() not in stopwords_set and word[1] not in punctuation_set and len(word[1].strip())>0:\n",
    "                word[tag_index] = word[tag_index].lower().strip().strip(punctuation)\n",
    "                cleaned_sent.append(word)\n",
    "        cleaned_sents.append(cleaned_sent)\n",
    "    return cleaned_sents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_and_clean_corpus(tag_index, file_name='wikicorpus.txt'):\n",
    "    sents = read_data(file_name)\n",
    "    cleaned_tagged_sents = clean_sents(sents, tag_index)\n",
    "    return cleaned_tagged_sents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at lemmas of just nouns for our targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_tokenizer(s):\n",
    "    return s.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_word_count_index(sents, min_frequency):\n",
    "    text = [\" \".join(sent) for sent in sents]\n",
    "    vectorizer = CountVectorizer(ngram_range=(1,1), tokenizer=my_tokenizer, #use my own tokenizer so it doesn't split on -\n",
    "                               token_pattern='(?u)\\b\\S.*\\b',                #use my own token pattern to accept hyphenated words\n",
    "                               min_df=min_frequency)                        #ignore words that don't occur at least 10 times\n",
    "    \n",
    "    vectorizer.fit_transform(text)\n",
    "    return vectorizer\n",
    "\n",
    "def get_target_words_index(cleaned_tagged_sents, tag_index, min_frequency_target_words=50):\n",
    "    target_sents = [[word[tag_index] for word in sent\n",
    "                  if word[2].startswith('N')] for sent in cleaned_tagged_sents]\n",
    "    return get_word_count_index(target_sents, min_frequency_target_words)\n",
    "\n",
    "def remove_tags_and_infrequent_words(cleaned_tagged_sents, tag_index, min_frequency_context_words=20):\n",
    "    corpus_sents = [[word[tag_index] for word in sent] for sent in cleaned_tagged_sents] \n",
    "    vectorizer_corpus = get_word_count_index(corpus_sents, min_frequency_context_words)\n",
    "    corpus_words = set([corpus_sent for corpus_sent in chain.from_iterable(corpus_sents)])\n",
    "    words_to_filter = corpus_words.difference(vectorizer_corpus.vocabulary_.keys())\n",
    "    filtered_corpus_sents = [[word for word in sent if word not in words_to_filter] for sent in corpus_sents]\n",
    "    return filtered_corpus_sents, vectorizer_corpus\n",
    "\n",
    "def create_collocation_matrix_with_window(filtered_corpus_sents, vectorizer_corpus, vectorizer_target, window_size=2):\n",
    "    filtered_vocab_length =  len(vectorizer_corpus.vocabulary_)\n",
    "    rows = filtered_vocab_length\n",
    "    cols = filtered_vocab_length\n",
    "    target_words = set(vectorizer_target.vocabulary_.keys())\n",
    "    target_context_matrix = lil_matrix((rows, cols), dtype = np.int)\n",
    "    for context_sent in filtered_corpus_sents:\n",
    "        for target_index, target_word in enumerate(context_sent):\n",
    "            if(target_word in target_words):\n",
    "                #AFTER context\n",
    "                for i in xrange(1,window_size+1):\n",
    "                    if(target_index+i) >= len(context_sent):\n",
    "                        break #reached end of sentence!\n",
    "                    else:\n",
    "                        context_word = context_sent[target_index+i]\n",
    "                        target_context_matrix[vectorizer_corpus.vocabulary_.get(target_word),\n",
    "                                              vectorizer_corpus.vocabulary_.get(context_word)] += 1\n",
    "                #BEFORE context\n",
    "                for i in xrange(1,window_size+1):\n",
    "                    if(target_index-i) < 0:\n",
    "                        break #reached end of sentence!\n",
    "                    else:\n",
    "                        context_word = context_sent[target_index-i]\n",
    "                        target_context_matrix[vectorizer_corpus.vocabulary_.get(target_word),\n",
    "                                              vectorizer_corpus.vocabulary_.get(context_word)] += 1\n",
    "    return target_context_matrix\n",
    "\n",
    "def remove_empty_rows_columns(target_context_matrix, vectorizer_corpus):\n",
    "    print \"original shape: \", target_context_matrix.shape\n",
    "    # remove the rows        \n",
    "    target_row_sums = target_context_matrix.sum(axis = 1) #sum across the rows\n",
    "    target_word_index_no_zeros = []\n",
    "    target_vocab_no_zeros = []\n",
    "    dropped_target_words = []\n",
    "    dropped_context_words = []\n",
    "    original_word_index = vectorizer_corpus.get_feature_names()\n",
    "    for word_index in range(0,len(original_word_index)):\n",
    "        if target_row_sums[ word_index ] != 0:\n",
    "            target_word_index_no_zeros.append(word_index)\n",
    "            target_vocab_no_zeros.append(original_word_index[word_index])\n",
    "        else:\n",
    "            dropped_target_words.append(original_word_index[word_index])\n",
    "    target_context_matrix_no_zeros = target_context_matrix[target_word_index_no_zeros, :]\n",
    "\n",
    "    # remove the columns\n",
    "    context_row_sums = target_context_matrix.sum(axis = 0) #sum down the columns\n",
    "    context_word_index_no_zeros = []\n",
    "    context_vocab_no_zeros = []\n",
    "    for word_index in range(0,len(original_word_index)):\n",
    "        if context_row_sums[0,word_index] != 0:\n",
    "            context_word_index_no_zeros.append(word_index)\n",
    "            context_vocab_no_zeros.append(original_word_index[word_index])\n",
    "        else:\n",
    "            dropped_context_words.append(original_word_index[word_index])\n",
    "    target_context_matrix_no_zeros = target_context_matrix_no_zeros[:, context_word_index_no_zeros]\n",
    "    \n",
    "    return target_context_matrix_no_zeros, target_vocab_no_zeros, dropped_target_words, dropped_context_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove rows and columns with zero values (to avoid problems with division later)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get collocations\n",
    "#### stop at sentence boundary\n",
    "* T x C matrix, where T=target words (rows) and C=context words (columns)  \n",
    "* use lil_matrix because it is efficient for building sparse matrices  \n",
    "* Note: We technically don't need to build a full co-occurence matrix because our target words are just the top 50 nouns, but for flexibliity we will build a C x C matrix so that we can easily switch to a different set of target words later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_collocation_matrix(cleaned_sents, tag_index):\n",
    "    vectorizer_target = get_target_words_index(cleaned_sents, tag_index)\n",
    "    filtered_corpus_sents, vectorizer_corpus = remove_tags_and_infrequent_words(cleaned_sents, tag_index)\n",
    "    target_context_matrix = create_collocation_matrix_with_window(filtered_corpus_sents, vectorizer_corpus, vectorizer_target)\n",
    "    target_context_matrix, target_word_index, dropped_target_words, dropped_context_words = remove_empty_rows_columns(target_context_matrix, vectorizer_corpus)\n",
    "    if(len(dropped_target_words) | len(dropped_context_words)):\n",
    "        print \"Dropped \", len(dropped_target_words), \" target words\"\n",
    "        print \"Dropped \", len(dropped_context_words), \" context words\"\n",
    "    return target_context_matrix, target_word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate association measures\n",
    "#### <font color='red'>Important:</font> need to convert our co-occurence lil matrix to a csr matrix beore we do any mathemtical operations!\n",
    "1. $ PMI(target,context) = log\\frac{P(target,context)}{P(target) P(context)} $  \n",
    "  * $P(target,context) = \\frac{count(target,context)}{count(\\_\\_,\\_\\_)}$  \n",
    "  * $P(target) = \\frac{count(target,\\_\\_)}{count(\\_\\_,\\_\\_)}$  \n",
    "  * $P(context) = \\frac{count(context,\\_\\_)}{count(\\_\\_,\\_\\_)}$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positive PMI: convert all negative numbers to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def calculate_ppmi(target_context_matrix):\n",
    "    target_context_matrix = target_context_matrix.tocsr()\n",
    "    count_all = target_context_matrix.sum()\n",
    "    count_target = target_context_matrix.sum(axis = 1) #rows\n",
    "    count_context = target_context_matrix.sum(axis = 0) #columns\n",
    "\n",
    "    prob_target = count_target / count_all\n",
    "    prob_context = count_context / count_all\n",
    "\n",
    "    #prob target and context\n",
    "    pmi_target_context_matrix = target_context_matrix / count_all\n",
    "\n",
    "    #divide by prob target\n",
    "    pmi_target_context_matrix = pmi_target_context_matrix / prob_target\n",
    "\n",
    "    #divide by prob context\n",
    "    pmi_target_context_matrix = pmi_target_context_matrix / prob_context\n",
    "\n",
    "    #take log -- this will generate a divide by zero warning because we are taking log of 0\n",
    "    pmi_target_context_matrix = np.log(pmi_target_context_matrix)\n",
    "    #replace all the -inf with large negative numbers\n",
    "    #pmi_target_context_matrix - np.nan_to_num(pmi_target_context_matrix)\n",
    "    \n",
    "    ppmi_target_context_matrix = np.maximum(pmi_target_context_matrix, 0)\n",
    "    return ppmi_target_context_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted jaccard:\n",
    "$j\\_sim (word_1, word_2) = \\frac{\\sum{min(word_1[dim_i], word_2[dim_i]})}{\\sum{max(word_1[dim_i], word_2[dim_i])}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def get_similarities_for_word(target_context_matrix, target_word_index, word_of_interest, sim_measure, top_k=20):\n",
    "    sims_list = []\n",
    "    target_word_to_index = dict(zip(target_word_index, list(range(0, len(target_word_index)))))\n",
    "    if (sim_measure == 'weighted_jaccard'):\n",
    "        sims_list = get_weighted_jaccard_for_word(target_context_matrix, target_word_to_index, word_of_interest)\n",
    "    elif(sim_measure == 'cosine'):\n",
    "        sims_list = get_cosine_for_word(target_context_matrix, target_word_to_index, word_of_interest) \n",
    "    else:\n",
    "        print \"Invalid sim measure!\"\n",
    "    sorted_sims = sorted(sims_list, key=lambda sim: sim[1], reverse=True)\n",
    "    return sorted_sims[:top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_weighted_jaccard_for_word(target_context_matrix, target_word_to_index, word_of_interest):\n",
    "    j_sims_list = []\n",
    "    index_word_of_interest = target_word_to_index.get(word_of_interest)\n",
    "    for target_word in target_word_to_index.keys():\n",
    "        if target_word != word_of_interest:\n",
    "            index_target_word = target_word_to_index.get(target_word)  \n",
    "            numerator = np.minimum(target_context_matrix[index_word_of_interest], target_context_matrix[index_target_word]).sum()\n",
    "            denominator = np.maximum(target_context_matrix[index_word_of_interest], target_context_matrix[index_target_word]).sum()\n",
    "            j_sims_list.append((target_word, (numerator/denominator)))\n",
    "    return j_sims_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine:\n",
    "$cosine\\_sim (word_1, word_2) = \\frac{word_1[dim_i] \\cdot word_2[dim_i]}{\\lVert word_1[dim_i]\\rVert \\lVert word_2[dim_i]\\rVert }$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_cosine_matrix(target_context_matrix):\n",
    "    target_context_matrix_norm = target_context_matrix / LA.norm(target_context_matrix, axis=0) #along rows\n",
    "    return np.dot(target_context_matrix_norm, target_context_matrix_norm.T)\n",
    "\n",
    "def get_cosine_for_word(target_context_matrix, target_word_to_index, word_of_interest):\n",
    "    cosine_target_context_matrix = get_cosine_matrix(target_context_matrix)\n",
    "    cos_sims_list = []\n",
    "    index_word_of_interest = target_word_to_index.get(word_of_interest)\n",
    "    for target_word in target_word_to_index.keys():\n",
    "        if target_word != word_of_interest:\n",
    "            index_target_word = target_word_to_index.get(target_word)\n",
    "            cos_sims_list.append((target_word,cosine_target_context_matrix[index_target_word,index_word_of_interest]))\n",
    "    return cos_sims_list     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in BLESS data set (tab-separated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_eval_data(file_name='BLESS_part.txt'):\n",
    "    with open(file_name, 'rb') as f:\n",
    "        bless_file = f.readlines()\n",
    "    bless_data = [line.split('\\t') for line in bless_file] # concept, class, relation, relatum\n",
    "    positive_pairs = [(data[0].split('-')[0], data[3].split('-')[0]) for data in bless_data if data[2]== \"coord\" or data[2]==\"hyper\"]\n",
    "    negative_pairs = [(data[0].split('-')[0], data[3].split('-')[0]) for data in bless_data if data[2]== \"mero\" or data[2]==\"random-n\"]\n",
    "    return positive_pairs, negative_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy @1 and @5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_scores(words_of_interest, target_context_matrix, target_word_index, sim_measure):\n",
    "    words_to_similarities = {}\n",
    "    words_to_scores = {}\n",
    "    accuracy_level = 5\n",
    "    \n",
    "    positive_pairs, _ = get_eval_data()\n",
    "    \n",
    "    for word_of_interest in words_of_interest:\n",
    "        similarities = get_similarities_for_word(target_context_matrix, target_word_index, word_of_interest, sim_measure)\n",
    "        words_to_similarities[word_of_interest] = similarities\n",
    "        accuracy_1 = 0\n",
    "        accuracy_5 = 0\n",
    "        for i in range(0,accuracy_level):\n",
    "            if (word_of_interest,similarities[i][0]) in positive_pairs:\n",
    "                if i==0:\n",
    "                    accuracy_1 = 1\n",
    "                accuracy_5 += 1    \n",
    "        accuracy_5 /= accuracy_level\n",
    "        words_to_scores[word_of_interest] = (accuracy_1, accuracy_5)\n",
    "    scores_array = np.asarray(words_to_scores.values())\n",
    "    print \"Average scores: \", np.mean(scores_array,axis=0)\n",
    "    print words_to_similarities\n",
    "    return np.mean(scores_array,axis=0), words_to_similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's put it all together now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_dist_space(assoc_measure='ppmi', tag_index=1): #lemma\n",
    "    cleaned_sents = load_and_clean_corpus(tag_index)\n",
    "    target_context_matrix, target_word_index = create_collocation_matrix(cleaned_sents, tag_index)\n",
    "    target_context_matrix_ppmi = calculate_ppmi(target_context_matrix)\n",
    "    return target_context_matrix_ppmi, target_word_index\n",
    "    \n",
    "def evaluate_similarities(words_of_interest, target_context_matrix, target_word_index, sim_measure):\n",
    "    words_to_scores,words_to_similarities  = get_scores(words_of_interest, target_context_matrix, target_word_index, sim_measure)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sims_list:  (u'writings', 0.10136481617129484)\n",
      "sims_list:  (u'writings', 0.051215023095576163)\n",
      "sims_list:  (u'writings', 0.090228070128985816)\n",
      "sims_list:  (u'writings', 0.094680146400720033)\n",
      "sims_list:  (u'writings', 0.056167671074115355)\n",
      "sims_list:  (u'writings', 0.057829853154266059)\n",
      "sims_list:  (u'writings', 0.10278926827553779)\n",
      "sims_list:  (u'writings', 0.072871001871896976)\n",
      "sims_list:  (u'writings', 0.11632275899904466)\n",
      "sims_list:  (u'writings', 0.054816483412950458)\n",
      "sims_list:  (u'writings', 0.14317646083261623)\n",
      "sims_list:  (u'writings', 0.12511553006254589)\n",
      "Average scores:  [ 0.41666667  0.21666667]\n",
      "{'horse': [(u'breed', 0.6195745019374066), (u'animal', 0.5946210061738183), (u'kill', 0.56012743285080957), (u'ride', 0.54663847333335369), (u'cavalry', 0.54633485612943677), (u'dog', 0.52682731299600438), (u'man', 0.51178903247703522), (u'camel', 0.50928663335608249), (u'back', 0.48482999212943323), (u'call', 0.47931225720605741), (u'cattle', 0.45453177447037613), (u'fight', 0.44290252940261327), (u'cat', 0.44080660861618043), (u'head', 0.4346135482244653), (u'mount', 0.43130649556642775), (u'ox', 0.42770612803598501), (u'foot', 0.42769352678344963), (u'turn', 0.42251981936329641), (u'long', 0.42237519014037267), (u'racing', 0.42066699065847524)], 'bomb': [(u'aircraft', 0.60870708539892515), (u'weapon', 0.60503221751227065), (u'bombing', 0.6046381595268302), (u'bomber', 0.56562902123476821), (u'attack', 0.54013073620361174), (u'fly', 0.5400119513917544), (u'missile', 0.47423161672371655), (u'target', 0.47304795899788393), (u'nuclear', 0.46272354912487823), (u'fire', 0.45963273730397669), (u'mission', 0.4569382952564458), (u'combat', 0.44853972016635463), (u'operation', 0.4435628211751384), (u'kill', 0.44191041659520658), (u'rocket', 0.42802373266577415), (u'flight', 0.42749488171253003), (u'war', 0.42156542285936399), (u'gun', 0.4146985022716313), (u'force', 0.40970593740873917), (u'air', 0.40753989843656702)], 'car': [(u'vehicle', 0.99202853723789441), (u'engine', 0.97215011570570709), (u'racing', 0.81314041964425365), (u'drive', 0.80988547044139647), (u'design', 0.78568285744474042), (u'company', 0.77359101486247817), (u'motor', 0.74490858510307323), (u'race', 0.7002809269530017), (u'use', 0.6714373116208785), (u'truck', 0.66686286987610699), (u'model', 0.66440625723799296), (u'run', 0.65428273254391778), (u'production', 0.6341447156585297), (u'passenger', 0.62344228734163087), (u'produce', 0.62331394694664755), (u'road', 0.61081418930420428), (u'train', 0.60950278808470248), (u'audi', 0.59079001781435625), (u'service', 0.58700122955587175), (u'type', 0.58305754207593452)], 'hospital': [(u'university', 0.69015057699079108), (u'school', 0.53437138545681917), (u'die', 0.50540854230770804), (u'college', 0.49104579511453283), (u'medical', 0.48887961612515585), (u'city', 0.47786645279614703), (u'health', 0.47545773778090628), (u'centre', 0.46449062034576383), (u'care', 0.46411960041198336), (u'london', 0.4582643647658764), (u'home', 0.4375440654406702), (u'town', 0.43751247576879776), (u'center', 0.43227744229080045), (u'service', 0.43155982588934927), (u'patient', 0.42804498610148317), (u'near', 0.41563221292909602), (u'visit', 0.41253267518076026), (u'institute', 0.40717862400984511), (u'county', 0.40279999708448067), (u'robert', 0.40191588534581474)], 'hotel': [(u'restaurant', 0.52873019451120185), (u'street', 0.52671824576815662), (u'park', 0.50599349005812455), (u'building', 0.50517553915713365), (u'house', 0.4916032553344607), (u'centre', 0.45306400699691685), (u'room', 0.44048979251590131), (u'london', 0.4317415706936194), (u'city', 0.42843246103082999), (u'home', 0.4219836256075315), (u'town', 0.41805554214472551), (u'apartment', 0.41628195285589253), (u'area', 0.41219156652653582), (u'near', 0.39542327421737056), (u'resort', 0.39469702381479288), (u'san', 0.39331183614507581), (u'several', 0.3919126751984387), (u'village', 0.38797593746407782), (u'visit', 0.37326715831051621), (u'tourist', 0.3714177498294251)], 'fox': [(u'dog', 0.44576837814008363), (u'television', 0.37496258054401704), (u'wild', 0.35108795639327917), (u'live', 0.34554363099767216), (u'film', 0.34472957291891115), (u'wolf', 0.34002036722457102), (u'movie', 0.3398197471130725), (u'bear', 0.33941626652238738), (u'former', 0.33902069471367657), (u'show', 0.33656402220361237), (u'coyote', 0.3363079444611346), (u'season', 0.33431778880981056), (u'tv', 0.32838476445918846), (u'red', 0.3269088565685786), (u'broadcast', 0.32681552736085379), (u'director', 0.32264507612607557), (u'turn', 0.32165944791286183), (u'cbs', 0.31327060512326677), (u'series', 0.31106139569213476), (u'animal', 0.30904741410900893)], 'gun': [(u'tank', 0.91407239145271957), (u'weapon', 0.90792224956067813), (u'fire', 0.89094662523256041), (u'mount', 0.85249676037012079), (u'armour', 0.78931369436873766), (u'cannon', 0.77847833555228763), (u'ship', 0.70262431510943579), (u'artillery', 0.66061146374926571), (u'rifle', 0.64976819547253262), (u'cruiser', 0.63836919178024654), (u'missile', 0.63641711174905491), (u'vehicle', 0.63363470542217992), (u'arm', 0.63057520993301341), (u'turret', 0.61839158387454429), (u'machine', 0.60228163614316299), (u'destroyer', 0.59515118400653555), (u'mm', 0.59210441797724933), (u'aircraft', 0.58422722209937916), (u'shell', 0.57251147956224091), (u'torpedo', 0.55875088546551299)], 'guitar': [(u'bass', 1.1656717093306646), (u'instrument', 0.87312725529895896), (u'play', 0.86344636695697308), (u'sound', 0.73265691459657989), (u'band', 0.70331761214061062), (u'string', 0.67097607509773105), (u'song', 0.66790615674530363), (u'vocal', 0.65307461530015576), (u'style', 0.63225672013913681), (u'music', 0.62962919698716557), (u'drum', 0.59137168990609068), (u'keyboard', 0.56759816624265735), (u'guitarist', 0.56302377749249899), (u'piano', 0.54197577178709544), (u'jazz', 0.53769752732845655), (u'album', 0.53494163017110874), (u'tune', 0.53425541144553479), (u'bassist', 0.5277967474053703), (u'blues', 0.50635691087072432), (u'musician', 0.49475146310162171)], 'bus': [(u'station', 0.70212999933253228), (u'rail', 0.70011672157435578), (u'service', 0.68513152365700325), (u'train', 0.64676082811910163), (u'run', 0.64429424069627661), (u'route', 0.60092153133161585), (u'network', 0.58649981404416618), (u'system', 0.57285999934026632), (u'transport', 0.56361323445064682), (u'design', 0.54799230092772466), (u'passenger', 0.54149633526965679), (u'car', 0.52597807221658188), (u'main', 0.50292498205695668), (u'city', 0.50004077967287042), (u'chip', 0.49593402368030637), (u'railway', 0.49397171874361956), (u'airport', 0.49371941351488863), (u'line', 0.49059897994712753), (u'computer', 0.48591733585849611), (u'road', 0.46728751103778082)], 'table': [(u'use', 0.62329112418360166), (u'call', 0.61863842660006518), (u'element', 0.60597485556802033), (u'list', 0.60270089766750401), (u'example', 0.59390982392594693), (u'datum', 0.57300099103829349), (u'word', 0.52845679041877336), (u'structure', 0.52679375967542064), (u'may', 0.52138849687739097), (u'object', 0.51292741260677333), (u'i', 0.51193800844520798), (u'number', 0.50870334058553013), (u'e', 0.50629062534075642), (u'function', 0.50230397372099356), (u'mean', 0.50045419723504303), (u'common', 0.49805785754404919), (u'form', 0.49697408522170755), (u'type', 0.49150487520569952), (u'single', 0.48534168138898526), (u'g', 0.47840892299515569)], 'piano': [(u'bass', 0.73484745958672615), (u'violin', 0.67228453169431945), (u'music', 0.66601592330641668), (u'concerto', 0.66158432356562691), (u'sonata', 0.62923712831615797), (u'string', 0.59105102539525767), (u'composer', 0.55952821394879149), (u'play', 0.55209098885248364), (u'guitar', 0.54197577178709544), (u'piece', 0.53594378786766927), (u'instrument', 0.51809874283119484), (u'beethoven', 0.49659371320871415), (u'cello', 0.48750339868626186), (u'jazz', 0.48503396788029168), (u'solo', 0.4771254793880994), (u'debussy', 0.46183159014291569), (u'musical', 0.4388989254946889), (u'composition', 0.43861497638017932), (u'style', 0.4332779722945142), (u'trio', 0.42915790381973545)], 'bowl': [(u'super', 1.0134307071496711), (u'win', 0.63577356894439319), (u'game', 0.57227121509758383), (u'team', 0.54700739805265852), (u'play', 0.54558252377226657), (u'champion', 0.53488900625678792), (u'nfl', 0.51905462204164321), (u'playoff', 0.50948147363869056), (u'championship', 0.49460800651212639), (u'season', 0.48285715688527897), (u'quarterback', 0.43237003207328423), (u'league', 0.42099408728961418), (u'football', 0.41826545816781613), (u'coach', 0.4088071650727455), (u'defeat', 0.38592401728663933), (u'afl', 0.37462238074184584), (u'stadium', 0.3627865820964048), (u'john', 0.36239351311201651), (u'beat', 0.35852625553337442), (u'record', 0.35698831283740862)]}\n",
      "CPU times: user 5min 55s, sys: 38.4 s, total: 6min 33s\n",
      "Wall time: 3min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "words_of_interest = ['car', 'bus', 'hospital', 'hotel', 'gun', 'bomb', 'horse', 'fox', 'table', 'bowl', 'guitar', 'piano']\n",
    "#target_context_matrix, target_word_index = create_dist_space()\n",
    "evaluate_similarities(words_of_interest, target_context_matrix, target_word_index, \"cosine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
