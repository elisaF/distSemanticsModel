{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from itertools import chain \n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.collocations import *\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from string import punctuation\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Watermark - tool to help with reproducibility:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%install_ext https://raw.githubusercontent.com/rasbt/watermark/master/watermark/watermark.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last updated: Mon Jun 27 2016 16:08:30 CDT\n",
      "\n",
      "CPython 2.7.11\n",
      "IPython 4.0.3\n",
      "\n",
      "matplotlib 1.5.1\n",
      "numpy 1.10.1\n",
      "conda 4.0.8\n",
      "pandas 0.17.1\n",
      "nltk 3.0.3\n",
      "\n",
      "compiler   : GCC 4.2.1 (Apple Inc. build 5577)\n",
      "system     : Darwin\n",
      "release    : 15.5.0\n",
      "machine    : x86_64\n",
      "processor  : i386\n",
      "CPU cores  : 4\n",
      "interpreter: 64bit\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark -n -t -z -u -m -v -p matplotlib,numpy,conda,pandas,nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_name = '/Users/elisa/Documents/CompLing/compSemantics/HW3/wikicorpus.txt'\n",
    "\n",
    "with open(file_name, 'rb') as f:\n",
    "    lines = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['Anarchism', 'Anarchism', 'NNP', 'I-NP', 'O', 'N'], ['.', '.', '.', 'O', 'O', '.\\n']], [['Anarchism', 'Anarchism', 'NNP', 'I-NP', 'O', 'N'], ['is', 'be', 'VBZ', 'I-VP', 'O', '(S[dcl]\\\\NP)/NP'], ['a', 'a', 'DT', 'I-NP', 'O', 'NP[nb]/N'], ['political', 'political', 'JJ', 'I-NP', 'O', 'N/N'], ['philosophy', 'philosophy', 'NN', 'I-NP', 'O', 'N'], ['encompassing', 'encompass', 'VBG', 'I-VP', 'O', '(S[ng]\\\\NP)/NP'], ['theories', 'theory', 'NNS', 'I-NP', 'O', 'N'], ['and', 'and', 'CC', 'I-NP', 'O', 'conj'], ['attitudes', 'attitude', 'NNS', 'I-NP', 'O', 'N'], ['which', 'which', 'WDT', 'B-NP', 'O', '(NP\\\\NP)/(S[dcl]\\\\NP)'], ['consider', 'consider', 'VBP', 'I-VP', 'O', '((S[dcl]\\\\NP)/(S[to]\\\\NP))/NP'], ['the', 'the', 'DT', 'I-NP', 'O', 'NP[nb]/N'], ['state', 'state', 'NN', 'I-NP', 'O', 'N'], ['to', 'to', 'TO', 'I-VP', 'O', '(S[to]\\\\NP)/(S[b]\\\\NP)'], ['be', 'be', 'VB', 'I-VP', 'O', '(S[b]\\\\NP)/(S[adj]\\\\NP)'], ['unnecessary', 'unnecessary', 'JJ', 'I-ADJP', 'O', 'S[adj]\\\\NP'], [',', ',', ',', 'I-ADJP', 'O', ','], ['harmful', 'harmful', 'JJ', 'I-ADJP', 'O', 'S[adj]\\\\NP'], [',', ',', ',', 'I-ADJP', 'O', ','], ['and/', 'and/', 'JJ', 'I-ADJP', 'O', 'S[adj]\\\\NP'], ['or', 'or', 'CC', 'I-ADJP', 'O', 'conj'], ['undesirable', 'undesirable', 'JJ', 'I-ADJP', 'O', 'S[adj]\\\\NP'], ['.', '.', '.', 'O', 'O', '.\\n']], [['Specific', 'Specific', 'NNP', 'I-NP', 'O', 'N/N'], ['anarchists', 'anarchist', 'NNS', 'I-NP', 'O', 'N'], ['may', 'may', 'MD', 'I-VP', 'O', '(S[dcl]\\\\NP)/(S[b]\\\\NP)'], ['have', 'have', 'VB', 'I-VP', 'O', '(S[b]\\\\NP)/NP'], ['additional', 'additional', 'JJ', 'I-NP', 'O', 'N/N'], ['criteria', 'criterion', 'NNS', 'I-NP', 'O', 'N'], ['for', 'for', 'IN', 'I-PP', 'O', '(NP\\\\NP)/NP'], ['what', 'what', 'WP', 'I-NP', 'O', 'NP/(S[dcl]\\\\NP)'], ['constitutes', 'constitute', 'VBZ', 'I-VP', 'O', '(S[dcl]\\\\NP)/NP'], ['anarchism', 'anarchism', 'NN', 'I-NP', 'O', 'N'], [',', ',', ',', 'O', 'O', ','], ['and', 'and', 'CC', 'O', 'O', 'conj'], ['they', 'they', 'PRP', 'I-NP', 'O', 'NP'], ['often', 'often', 'RB', 'I-ADVP', 'O', '(S\\\\NP)/(S\\\\NP)'], ['disagree', 'disagree', 'VBP', 'I-VP', 'O', '(S[dcl]\\\\NP)/PP'], ['with', 'with', 'IN', 'I-PP', 'O', 'PP/NP'], ['each', 'each', 'DT', 'I-NP', 'O', 'NP[nb]/N'], ['other', 'other', 'NN', 'I-NP', 'O', 'N'], ['on', 'on', 'IN', 'I-PP', 'O', '((S\\\\NP)\\\\(S\\\\NP))/NP'], ['what', 'what', 'WP', 'I-NP', 'O', 'NP/(S[dcl]/NP)'], ['these', 'these', 'DT', 'B-NP', 'O', 'NP[nb]/N'], ['criteria', 'criterion', 'NNS', 'I-NP', 'O', 'N'], ['are', 'be', 'VBP', 'I-VP', 'O', '(S[dcl]\\\\NP)/NP'], ['.', '.', '.', 'O', 'O', '.\\n']]]\n",
      "CPU times: user 57.9 s, sys: 1min 15s, total: 2min 13s\n",
      "Wall time: 2min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sents = []\n",
    "for line in lines:\n",
    "    match = re.search(r'^<c> ', line)\n",
    "    if match:\n",
    "        line = line.decode('cp1252').encode('utf-8') #convert from unicode to utf8\n",
    "        tagged_words = [word_info for word_info in line[match.end(0):].split(\" \")]\n",
    "        sents.append([tagged_word.split('|') for tagged_word in tagged_words])\n",
    "print sents[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up the data\n",
    "#### Lower case, remove stop words, punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15min 17s, sys: 2min 24s, total: 17min 42s\n",
      "Wall time: 17min 47s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elisa/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:4: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cleaned_sents = [[word for word in sent \n",
    "                  if word[0].lower() not in stopwords.words('english')\n",
    "                  and word[0] not in punctuation\n",
    "                 ] for sent in sents] \n",
    "cleaned_sents[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at lemmas of just nouns for our targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['anarchism'],\n",
       " ['anarchism', 'philosophy', 'theory', 'attitude', 'state'],\n",
       " ['specific', 'anarchist', 'criterion', 'anarchism', 'criterion']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_index = 1 #lemma\n",
    "target_sents = [[word[tag_index].lower() for word in sent \n",
    "                  if word[2].startswith('N')\n",
    "                 ] for sent in cleaned_sents] \n",
    "target_sents[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the top 50 most frequent noun lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target_text = [\" \".join(target_sent) for target_sent in target_sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'time', 14192L), (u'year', 13251L), (u'system', 9949L), (u'city', 9743L), (u'number', 9571L), (u'world', 9144L), (u'state', 8573L), (u'part', 7927L), (u'example', 7233L), (u'century', 7180L), (u'war', 7070L), (u'name', 6858L), (u'people', 6851L), (u'group', 6796L), (u'country', 6772L), (u'area', 6441L), (u'language', 6406L), (u'work', 6292L), (u'united', 6174L), (u'use', 6128L), (u'government', 6094L), (u'game', 6077L), (u'term', 5822L), (u'form', 5736L), (u'book', 5511L), (u'life', 5452L), (u'church', 5433L), (u'day', 5106L), (u'member', 4991L), (u'case', 4882L), (u'new', 4816L), (u'film', 4755L), (u'word', 4750L), (u'states', 4693L), (u'law', 4690L), (u'force', 4689L), (u'history', 4623L), (u'power', 4615L), (u'man', 4503L), (u'order', 4459L), (u'point', 4458L), (u'school', 4425L), (u'way', 4360L), (u'series', 4154L), (u'death', 4130L), (u'line', 4097L), (u'population', 4097L), (u'team', 4018L), (u'text', 3964L), (u'end', 3944L)]\n",
      "CPU times: user 6.48 s, sys: 270 ms, total: 6.75 s\n",
      "Wall time: 6.83 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "v_target = CountVectorizer(ngram_range=(1,1))\n",
    "unigram_matrix = v_target.fit_transform(target_text)\n",
    "#sort\n",
    "features_count = unigram_matrix.sum(axis=0).tolist()[0]\n",
    "features_names = v_target.get_feature_names()\n",
    "sorted_counts = sorted(zip(features_names, features_count), key=lambda count: count[1], reverse=True)\n",
    "top_50 = sorted_counts[:50]\n",
    "print top_50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How big is our  vocabulary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151372\n"
     ]
    }
   ],
   "source": [
    "target_words = set([target_sent for target_sent in chain.from_iterable(target_sents)])\n",
    "print len(target_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "217953\n"
     ]
    }
   ],
   "source": [
    "context_sents = [[word[tag_index].lower() for word in sent] for sent in cleaned_sents] \n",
    "context_words = set([context_sent for context_sent in chain.from_iterable(context_sents)])\n",
    "print len(context_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get collocations\n",
    "#### stop at sentence boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 558210 samples and 2670887 outcomes>\n",
      "CPU times: user 1min 47s, sys: 1.35 s, total: 1min 48s\n",
      "Wall time: 1min 51s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elisa/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:8: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "window_size = 5\n",
    "bigramFreqDist = FreqDist()\n",
    "for target_word in top_50:\n",
    "    #target_word = 'anarchism'\n",
    "    #print \"target word \", target_word[0]\n",
    "    for context_sent in context_sents:\n",
    "        #print \"context sent \", context_sent\n",
    "        context_indices = [context_index for context_index, context_word in enumerate(context_sent) if context_word == target_word[0]]\n",
    "        #print \"context indices \", context_indices\n",
    "        for context_index in context_indices:\n",
    "            #AFTER\n",
    "            for i in xrange(1,window_size+1):\n",
    "                if(context_index+i) >= len(context_sent):\n",
    "                    #print \"end of sentence!\"\n",
    "                    break\n",
    "                else:\n",
    "                    context_word = context_sent[context_index+i]\n",
    "                    #print \"context_word\", context_word\n",
    "                    bigramFreqDist[(target_word[0],context_word)] += 1\n",
    "            #BEFORE\n",
    "            for i in xrange(1,window_size+1):\n",
    "                if(context_index-i) < 0:\n",
    "                    #print \"before end of sentence!\"\n",
    "                    break\n",
    "                else:\n",
    "                    context_word = context_sent[context_index-i]\n",
    "                    #print \"before context_word\", context_word\n",
    "                    bigramFreqDist[(target_word[0],context_word)] += 1\n",
    "print bigramFreqDist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate association measures\n",
    "1. PMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'use', 28535L), (u'also', 24074L), (u'one', 22231L), (u'first', 17037L), (u'time', 14991L), (u'include', 14559L), (u'year', 14148L), (u'two', 14126L), (u'many', 13470L), (u'make', 13016L), (u'new', 12436L), (u'may', 11720L), (u'would', 10833L), (u'become', 10704L), (u'state', 10522L), (u'system', 9967L), (u'number', 9860L), (u'city', 9804L), (u'however', 9794L), (u'name', 9638L), (u'form', 9623L), (u'know', 9535L), (u'work', 9431L), (u'call', 9392L), (u'world', 9312L), (u'take', 8610L), (u'see', 8079L), (u'part', 8061L), (u'give', 7768L), (u'century', 7482L), (u'war', 7327L), (u'well', 7320L), (u'example', 7240L), (u'later', 7204L), (u'often', 7130L), (u'group', 6975L), (u'people', 6869L), (u'country', 6833L), (u'early', 6732L), (u'three', 6726L), (u'since', 6704L), (u'language', 6600L), (u'term', 6534L), (u'write', 6496L), (u'follow', 6483L), (u'find', 6473L), (u'area', 6467L), (u'united', 6254L), (u'game', 6210L), (u'government', 6200L)]\n",
      "CPU times: user 10.1 s, sys: 601 ms, total: 10.7 s\n",
      "Wall time: 10.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "context_text = [\" \".join(context_sent) for context_sent in context_sents]\n",
    "v_context = CountVectorizer(ngram_range=(1,1))\n",
    "unigram_matrix = v_context.fit_transform(context_text)\n",
    "#sort\n",
    "features_count = unigram_matrix.sum(axis=0).tolist()[0]\n",
    "features_names = v_context.get_feature_names()\n",
    "sorted_counts = sorted(zip(features_names, features_count), key=lambda count: count[1], reverse=True)\n",
    "top_50 = sorted_counts[:50]\n",
    "print top_50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "for(bigram in bigramFreqDist.keys()):\n",
    "    target_word = bigram[0]\n",
    "    context_word = bigram[1]\n",
    "    joint_prob = bigramFreqDist.get(bigram) / features_count[v_context.vocabulary_.get(target_word)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "143632"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_context.vocabulary_.get('time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14991L"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "143632]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
